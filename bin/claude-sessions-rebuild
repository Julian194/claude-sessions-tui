#!/usr/bin/env bash
set -e

CLAUDE_DIR="${CLAUDE_DIR:-$HOME/.claude}"
CACHE="$CLAUDE_DIR/sessions-cache.tsv"
PROJECTS="$CLAUDE_DIR/projects"

# Get the directory where this script is located
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Ensure cache directory exists
mkdir -p "$(dirname "$CACHE")"

# Temporary files
TEMP_NEW="$CLAUDE_DIR/.cache-new-$$"
TEMP_MERGED="$CLAUDE_DIR/.cache-merged-$$"
TEMP_SORTED="$CLAUDE_DIR/.cache-sorted-$$"
TEMP_FULL="$CLAUDE_DIR/.cache-full-$$"
TEMP_ROOTS="$CLAUDE_DIR/.cache-roots-$$"
TEMP_BRANCHES="$CLAUDE_DIR/.cache-branches-$$"
trap 'rm -f "$TEMP_NEW" "$TEMP_MERGED" "$TEMP_MERGED.ids" "$TEMP_SORTED" "$TEMP_FULL" "$TEMP_ROOTS" "$TEMP_BRANCHES"' EXIT

# Inline extraction function (faster than calling external script for each file)
extract_meta() {
  local f="$1"
  [ ! -f "$f" ] && return
  local sid proj date mtime summary parent_sid
  sid=$(basename "$f" .jsonl)
  proj=$(basename "$(dirname "$f")" | sed "s/-Users-[^-]*-code-//")
  if [[ "$OSTYPE" == "darwin"* ]]; then
    date=$(stat -f "%Sm" -t "%m-%d" "$f")
    mtime=$(stat -f "%m" "$f")
  else
    date=$(date -r "$f" "+%m-%d")
    mtime=$(stat -c "%Y" "$f")
  fi
  summary=$(head -c 50000 "$f" | grep -m1 -o '"summary":"[^"]*"' | sed 's/"summary":"//;s/"$//' || true)
  [ -z "$summary" ] && summary="-"
  # Extract parent session ID if this is a branched session
  parent_sid=$(head -c 1000 "$f" | grep -m1 -o '"parentSession":"[^"]*"' | sed 's/"parentSession":"//;s/"$//' || true)
  printf '%s\t%s\t%s\t%s\t%s\t%s\n' "$sid" "$date" "$proj" "$summary" "$mtime" "$parent_sid"
}
export -f extract_meta

# Check if incremental update is possible (cache exists, has content, and has mtime/parent column)
incremental=false
if [ -f "$CACHE" ] && [ -s "$CACHE" ]; then
  # Check if cache has 5 or 6 columns (format with mtime, optionally parent_sid)
  cols=$(head -1 "$CACHE" | awk -F'\t' '{print NF}')
  if [ "$cols" -ge 5 ]; then
    incremental=true
  fi
fi

# ANSI color codes for branch display
CYAN='\033[0;36m'
NC='\033[0m'

# Group branches under their parents
group_branches() {
  local input="$1"

  # Separate root sessions (no parent) and branches (have parent)
  awk -F'\t' '$6 == "" || $6 == "-" { print }' "$input" > "$TEMP_ROOTS"
  awk -F'\t' '$6 != "" && $6 != "-" { print }' "$input" > "$TEMP_BRANCHES"

  # For each root session, output it followed by its branches
  while IFS=$'\t' read -r sid date proj summary mtime parent_sid; do
    # Output the root session
    printf '%s\t%s\t%s\t%s\t%s\t%s\n' "$sid" "$date" "$proj" "$summary" "$mtime" "$parent_sid"

    # Find and output any branches of this session (with visual formatting)
    while IFS=$'\t' read -r b_sid b_date b_proj b_summary b_mtime b_parent; do
      if [ "$b_parent" = "$sid" ]; then
        # Format branch with indentation and color
        printf '%s\t%b  └─ %s%b\t%s\t%s\t%s\t%s\n' "$b_sid" "$CYAN" "$b_date" "$NC" "$b_proj" "$b_summary" "$b_mtime" "$b_parent"
      fi
    done < "$TEMP_BRANCHES"
  done < "$TEMP_ROOTS"
}

if $incremental; then
  # Find only files newer than cache
  newer_files=$(find "$PROJECTS" -name "*.jsonl" -newer "$CACHE" ! -name "agent-*" 2>/dev/null || true)

  if [ -z "$newer_files" ]; then
    # No changes - output grouped cache
    group_branches "$CACHE"
    exit 0
  fi

  # Count new files for parallel decision
  new_count=$(echo "$newer_files" | wc -l | tr -d ' ')

  if [ "$new_count" -gt 10 ]; then
    # Many files: use parallel processing
    echo "$newer_files" | xargs -P 8 -I {} bash -c 'extract_meta "$@"' _ {} > "$TEMP_NEW" 2>/dev/null || true
  else
    # Few files: sequential is faster (less overhead)
    echo "$newer_files" | while read -r f; do
      extract_meta "$f"
    done > "$TEMP_NEW"
  fi

  # Get list of updated session IDs
  updated_ids=$(cut -f1 "$TEMP_NEW")

  # Filter old cache: keep entries that weren't updated
  if [ -n "$updated_ids" ]; then
    echo "$updated_ids" > "$TEMP_MERGED.ids"
    grep -v -F -f "$TEMP_MERGED.ids" "$CACHE" > "$TEMP_MERGED" 2>/dev/null || true
  else
    cp "$CACHE" "$TEMP_MERGED"
  fi

  # Merge, sort by mtime (column 5, descending = newest first), and group branches
  cat "$TEMP_NEW" "$TEMP_MERGED" | sort -t$'\t' -k5 -rn | awk -F'\t' '!seen[$1]++' > "$TEMP_SORTED"
  cp "$TEMP_SORTED" "$CACHE"
  group_branches "$TEMP_SORTED"

else
  # Full rebuild - sequential (faster due to less overhead)
  if [[ "$OSTYPE" == "darwin"* ]]; then
    ls -t "$PROJECTS"/*/*.jsonl 2>/dev/null | grep -v 'agent-'
  else
    find "$PROJECTS" -name "*.jsonl" -printf '%T@ %p\n' 2>/dev/null | \
      sort -rn | cut -d' ' -f2- | grep -v 'agent-'
  fi | while read -r f; do
    extract_meta "$f"
  done | awk -F'\t' '!seen[$1]++' > "$TEMP_FULL"
  cp "$TEMP_FULL" "$CACHE"
  group_branches "$TEMP_FULL"
fi
